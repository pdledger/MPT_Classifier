{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d641378",
   "metadata": {},
   "source": [
    "**The first thing to do is to run Creator.py to setup the dataset. First specify the options**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be154fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The Name of the Dataset\n",
    "Name = \"British-Coins-Updated-1p-2p/Coins-100\"\n",
    "\n",
    "# Frequency Array\n",
    "Frequencies=np.array([119.25,178.875,238.5,298.125,357.75,477,596.25,715.5,954,1192.5,1431,1908,2385,2862,3816,4770,5724,7632,9540,12402,16218,20988,26712,34344,43884,57240,73458,95400])\n",
    "Frequencies = Frequencies*6.28 #Convert to rad/s'\n",
    "\n",
    "# Classes to Include\n",
    "Classes = [\"British_Coins_Updated_1p_2p\"]\n",
    "\n",
    "# Global number of results\n",
    "# Coin Problem\n",
    "extend_results = 'global_obj'\n",
    "\n",
    "# Number of secondary results if 'global_obj'\n",
    "\n",
    "#Coin Problem`\n",
    "Num_Results = 100\n",
    "\n",
    "# Classes if 'global_class' or 'classwise'\n",
    "class_split=[]\n",
    "\n",
    "# Name each of the classes this is done in order\n",
    "class_names = []\n",
    "\n",
    "# Number of results if 'global_class'\n",
    "Num_Results_class = []\n",
    "\n",
    "\n",
    "# (int) Number of results per class results per class if 'classwise'\n",
    "class_num_results = []\n",
    "\n",
    "#Labeler False,'Classwise', 'Objectwise'\n",
    "Label_Data = 'Objectwise'\n",
    "Name_Objects = True\n",
    "\n",
    "#(dictionary) name the objects as you wish them to appear in the classificaiton\n",
    "Object_Names_dictionary ={\"Two_Pound\":r\"£2\", \"Ten_Pence\":r\"10p_(new)\", \"Ten_Pence_non_magnetic\":r\"10p_(old)\", \"One_Pound\":r\"£1\", \"Two_Penny\":r\"2p_(new)\",\n",
    "                              \"Two_Penny_non_magnetic\":r\"2p_(old)\", \"Twenty_Pence\":r\"20p\", \"Five_Pence\":r\"5p_(new)\", \"Five_Pence_non_magnetic\":r\"5p_(old)\",\n",
    "                              \"Fifty_Pence\":r\"50p\", \"One_Penny\":r\"1p_(new)\", \"One_Penny_non_magnetic\":r\"1p_(old)\"}\n",
    "\n",
    "\n",
    "Name_Order = [\"One_Penny\",\"One_Penny_non_magnetic\",\"Two_Penny\",\"Two_Penny_non_magnetic\",\"Five_Pence\",\"Five_Pence_non_magnetic\",\n",
    "             \"Ten_Pence\",\"Ten_Pence_non_magnetic\",\"Twenty_Pence\",\"Fifty_Pence\",\"One_Pound\",\"Two_Pound\"]\n",
    "\n",
    "#How to scale\n",
    "Scale_type = 'Global'\n",
    "\n",
    "#Which file (This is not currently used)\n",
    "Scale_File = 'Coin_DataSet.csv'\n",
    "\n",
    "\n",
    "#Alpha scale\n",
    "Alpha_scale = 0.84\n",
    "\n",
    "#Sigma scale\n",
    "Sigma_scale = 12.5\n",
    "\n",
    "# Create a dictionary of these settings\n",
    "Creator_Settings = {\"Name\":Name,\"Frequencies\":Frequencies,\"Classes\":Classes,\"extend_results\":extend_results,\"Num_Results\":Num_Results,#\n",
    "                    \"class_split\":class_split,\"class_names\":class_names,\"Num_Results_class\":Num_Results_class,\"class_num_results\":class_num_results,\\\n",
    "                    \"Label_Data\":Label_Data,\"Name_Objects\":Name_Objects,\"Object_Names_dictionary\":Object_Names_dictionary,\"Name_Order\":Name_Order,\\\n",
    "                    \"Scale_type\":Scale_type,\"Scale_File\":Scale_File,\"Alpha_scale\":Alpha_scale,\"Sigma_scale\":Sigma_scale}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf2c8608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder path to the dataset is: British-Coins-Updated-1p-2p/Coins-100_Al_0.84_Sig_12.5\n"
     ]
    }
   ],
   "source": [
    "# Run the Creator script\n",
    "from Creator import *\n",
    "#DataSet_Name=Creator(Name,Frequencies,Classes,extend_results,Num_Results,class_split,class_names,\\\n",
    "#            Num_Results_class,class_num_results,Label_Data, Name_Objects,Object_Names_dictionary,Name_Order,Scale_type,Scale_File,\\\n",
    "#            Alpha_scale,Sigma_scale)\n",
    "DataSet_Name=Creator(Creator_Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96be8df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British-Coins-Updated-1p-2p/Coins-100_Al_0.84_Sig_12.5\n"
     ]
    }
   ],
   "source": [
    "# This is the dataset name to be used with the classifier\n",
    "print(DataSet_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb993227",
   "metadata": {},
   "source": [
    "**The next part involves running the actual classifier. This is done using Trainer_PDL.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adca0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to load external testing data from disk. Requires that external_file_loader.py be run first.\n",
    "Load_External_Data = False \n",
    "# Option to plot comparison figures between the input array of simulated data and the external test data.\n",
    "# Currently only supported for a single class test set.\n",
    "Plot_Comparison_Figures = False\n",
    "# Option to additionally save to disk: the model for each bootstrap iteration, the normalisation coefficients for each,\n",
    "# bootstrap iteration, and the input array for each model, Used for debugging.\n",
    "Full_Save = False\n",
    "\n",
    "# Option to use SVD to reduce the number of features\n",
    "Reduce_Features = True\n",
    "\n",
    "#Model to be used\n",
    "#Optional models 'LogisticRegression', 'SVM', 'DecisionTree', 'RandomForest', 'GradientBoost', 'MLP','MLP,(n1,n2,...,nn)'\n",
    "Models_to_run = ['LogisticRegression','SVM','GradientBoost']\n",
    "\n",
    "#Features\n",
    "Features = ['Pri1', 'Pri2', 'Pri3']\n",
    "# Features = ['Eig1', 'Eig2', 'Eig3']\n",
    "#(list) list of features to be used options:\n",
    "#'Eig1','Eig2','Eig3','Pri1','Pri2','Pri3','Dev2','Dev3','Com'\n",
    "#Eigenvalues, Principal invarients, Deviatoric invarients, Comutator\n",
    "\n",
    "#How many times would you like to train the model\n",
    "Bootstrap_Repetitions = 1\n",
    "#(int) how many times to train the model to obtain an average accuracy\n",
    "\n",
    "# use default levels of SNR\n",
    "SNR_array=[]\n",
    "\n",
    "Trainer_Settings = {\"DataSet_Name\": DataSet_Name, \"Load_External_Data\":Load_External_Data, \"Plot_Comparison_Figures\":Plot_Comparison_Figures,\\\n",
    "                   \"Full_Save\": Full_Save, \"Reduce_Features\":Reduce_Features, \"Models_to_run\": Models_to_run, \"Features\":Features, \"Bootstrap_Repetitions\": Bootstrap_Repetitions, \"SNR_array\": SNR_array }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d510a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:56:34.158979: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-02 15:56:34.160820: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-02 15:56:34.193573: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-02 15:56:34.194283: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 15:56:34.860381: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level =  5\n",
      "[1.25301783e-08 1.33868643e-08 1.13078191e-08 ... 4.64711087e-07\n",
      " 3.16622386e-07 5.22180092e-07]\n",
      "to here\n",
      "LogisticRegression\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  108\n",
      "(900, 108)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "SVM\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  108\n",
      "(900, 108)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "GradientBoost\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  108\n",
      "(900, 108)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  10\n",
      "[1.25301783e-08 1.33868643e-08 1.13078191e-08 ... 4.64711087e-07\n",
      " 3.16622386e-07 5.22180092e-07]\n",
      "to here\n",
      "LogisticRegression\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  107\n",
      "(900, 107)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "SVM\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  107\n",
      "(900, 107)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "GradientBoost\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  107\n",
      "(900, 107)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  15\n",
      "[1.25301783e-08 1.33868643e-08 1.13078191e-08 ... 4.64711087e-07\n",
      " 3.16622386e-07 5.22180092e-07]\n",
      "to here\n",
      "LogisticRegression\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  103\n",
      "(900, 103)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "SVM\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  103\n",
      "(900, 103)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "GradientBoost\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  103\n",
      "(900, 103)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  20\n",
      "[1.25301783e-08 1.33868643e-08 1.13078191e-08 ... 4.64711087e-07\n",
      " 3.16622386e-07 5.22180092e-07]\n",
      "to here\n",
      "LogisticRegression\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  99\n",
      "(900, 99)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "SVM\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  99\n",
      "(900, 99)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "GradientBoost\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  99\n",
      "(900, 99)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  40\n",
      "[1.25301783e-08 1.33868643e-08 1.13078191e-08 ... 4.64711087e-07\n",
      " 3.16622386e-07 5.22180092e-07]\n",
      "to here\n",
      "LogisticRegression\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  61\n",
      "(900, 61)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "SVM\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  61\n",
      "(900, 61)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "GradientBoost\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  61\n",
      "(900, 61)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n"
     ]
    }
   ],
   "source": [
    "from Trainer_PDL import *\n",
    "\n",
    "#main(DataSet_Name,Load_External_Data,Plot_Comparison_Figures,Full_Save,Reduce_Features,Models_to_run,Features,Bootstrap_Repetitions)\n",
    "main(Trainer_Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b5a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
