{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1826d0f2",
   "metadata": {},
   "source": [
    "**The first thing to do is to run Creator.py to setup the dataset. First specify the options**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d35fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The Name of the Dataset\n",
    "Name = \"British-Coins-Updated-1p-2p/Coins-100\"\n",
    "\n",
    "# Frequency Array\n",
    "Frequencies=np.array([119.25,178.875,238.5,298.125,357.75,477,596.25,715.5,954,1192.5,1431,1908,2385,2862,3816,4770,5724,7632,9540,12402,16218,20988,26712,34344,43884,57240,73458,95400])\n",
    "Frequencies = Frequencies*6.28 #Convert to rad/s'\n",
    "\n",
    "# Classes to Include\n",
    "Classes = [\"British_Coins_Updated_1p_2p\"]\n",
    "\n",
    "# Global number of results\n",
    "# Coin Problem\n",
    "extend_results = 'global_obj'\n",
    "\n",
    "# Number of secondary results if 'global_obj'\n",
    "\n",
    "#Coin Problem`\n",
    "Num_Results = 100\n",
    "\n",
    "# Classes if 'global_class' or 'classwise'\n",
    "class_split=[]\n",
    "\n",
    "# Name each of the classes this is done in order\n",
    "class_names = []\n",
    "\n",
    "# Number of results if 'global_class'\n",
    "Num_Results_class = []\n",
    "\n",
    "\n",
    "# (int) Number of results per class results per class if 'classwise'\n",
    "class_num_results = []\n",
    "\n",
    "#Labeler False,'Classwise', 'Objectwise'\n",
    "Label_Data = 'Objectwise'\n",
    "Name_Objects = True\n",
    "\n",
    "#(dictionary) name the objects as you wish them to appear in the classificaiton\n",
    "Object_Names_dictionary ={\"Two_Pound\":r\"£2\", \"Ten_Pence\":r\"10p_(new)\", \"Ten_Pence_non_magnetic\":r\"10p_(old)\", \"One_Pound\":r\"£1\", \"Two_Penny\":r\"2p_(new)\",\n",
    "                              \"Two_Penny_non_magnetic\":r\"2p_(old)\", \"Twenty_Pence\":r\"20p\", \"Five_Pence\":r\"5p_(new)\", \"Five_Pence_non_magnetic\":r\"5p_(old)\",\n",
    "                              \"Fifty_Pence\":r\"50p\", \"One_Penny\":r\"1p_(new)\", \"One_Penny_non_magnetic\":r\"1p_(old)\"}\n",
    "\n",
    "\n",
    "Name_Order = [\"One_Penny\",\"One_Penny_non_magnetic\",\"Two_Penny\",\"Two_Penny_non_magnetic\",\"Five_Pence\",\"Five_Pence_non_magnetic\",\n",
    "             \"Ten_Pence\",\"Ten_Pence_non_magnetic\",\"Twenty_Pence\",\"Fifty_Pence\",\"One_Pound\",\"Two_Pound\"]\n",
    "\n",
    "#How to scale\n",
    "Scale_type = 'Global'\n",
    "\n",
    "#Which file (This is not currently used)\n",
    "Scale_File = 'Coin_DataSet.csv'\n",
    "\n",
    "\n",
    "#Alpha scale\n",
    "Alpha_scale = 0.84\n",
    "\n",
    "#Sigma scale\n",
    "Sigma_scale = 12.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "131360fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder path to the dataset is: British-Coins-Updated-1p-2p/Coins-100_Al_0.84_Sig_12.5_Al_0.84_Sig_12.5\n"
     ]
    }
   ],
   "source": [
    "# Run the Creator script\n",
    "from Creator import *\n",
    "DataSet_Name=Creator(Name,Frequencies,Classes,extend_results,Num_Results,class_split,class_names,\\\n",
    "            Num_Results_class,class_num_results,Label_Data, Name_Objects,Object_Names_dictionary,Name_Order,Scale_type,Scale_File,\\\n",
    "            Alpha_scale,Sigma_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fece088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British-Coins-Updated-1p-2p/Coins-100_Al_0.84_Sig_12.5_Al_0.84_Sig_12.5\n"
     ]
    }
   ],
   "source": [
    "# This is the dataset name to be used with the classifier\n",
    "print(DataSet_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e3d91",
   "metadata": {},
   "source": [
    "**The next part involves running the actual classifier. This is done using Trainer_PDL.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bba2af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to load external testing data from disk. Requires that external_file_loader.py be run first.\n",
    "Load_External_Data = False \n",
    "# Option to plot comparison figures between the input array of simulated data and the external test data.\n",
    "# Currently only supported for a single class test set.\n",
    "Plot_Comparison_Figures = False\n",
    "# Option to additionally save to disk: the model for each bootstrap iteration, the normalisation coefficients for each,\n",
    "# bootstrap iteration, and the input array for each model, Used for debugging.\n",
    "Full_Save = False\n",
    "\n",
    "# Option to use SVD to reduce the number of features\n",
    "Reduce_Features = True\n",
    "\n",
    "#Model to be used\n",
    "#Optional models 'LogisticRegression', 'SVM', 'DecisionTree', 'RandomForest', 'GradientBoost', 'MLP','MLP,(n1,n2,...,nn)'\n",
    "Models_to_run = ['LogisticRegression']\n",
    "\n",
    "#Features\n",
    "Features = ['Pri1', 'Pri2', 'Pri3']\n",
    "# Features = ['Eig1', 'Eig2', 'Eig3']\n",
    "#(list) list of features to be used options:\n",
    "#'Eig1','Eig2','Eig3','Pri1','Pri2','Pri3','Dev2','Dev3','Com'\n",
    "#Eigenvalues, Principal invarients, Deviatoric invarients, Comutator\n",
    "\n",
    "#How many times would you like to train the model\n",
    "Bootstrap_Repetitions = 1\n",
    "#(int) how many times to train the model to obtain an average accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40b69f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level =  5\n",
      "[1.25301783e-08 1.11963438e-08 1.30787701e-08 ... 5.48437021e-07\n",
      " 4.80758995e-07 2.68876557e-07]\n",
      "to here\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  109\n",
      "(900, 109)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  10\n",
      "[1.25301783e-08 1.11963438e-08 1.30787701e-08 ... 5.48437021e-07\n",
      " 4.80758995e-07 2.68876557e-07]\n",
      "to here\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  106\n",
      "(900, 106)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  15\n",
      "[1.25301783e-08 1.11963438e-08 1.30787701e-08 ... 5.48437021e-07\n",
      " 4.80758995e-07 2.68876557e-07]\n",
      "to here\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  103\n",
      "(900, 103)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  20\n",
      "[1.25301783e-08 1.11963438e-08 1.30787701e-08 ... 5.48437021e-07\n",
      " 4.80758995e-07 2.68876557e-07]\n",
      "to here\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  99\n",
      "(900, 99)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n",
      "Noise level =  40\n",
      "[1.25301783e-08 1.11963438e-08 1.30787701e-08 ... 5.48437021e-07\n",
      " 4.80758995e-07 2.68876557e-07]\n",
      "to here\n",
      "900 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  61\n",
      "(900, 61)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 300  Results\n",
      "(300, 12) (300, 12) (300, 12)\n",
      "Completed snapshot classification\n"
     ]
    }
   ],
   "source": [
    "from Trainer_PDL import *\n",
    "\n",
    "main(DataSet_Name,Load_External_Data,Plot_Comparison_Figures,Full_Save,Reduce_Features,Models_to_run,Features,Bootstrap_Repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f73d7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
