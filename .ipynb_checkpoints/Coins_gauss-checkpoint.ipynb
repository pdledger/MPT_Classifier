{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d641378",
   "metadata": {},
   "source": [
    "**The first thing to do is to run Creator.py to setup the dataset. First specify the options**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be154fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The Name of the Dataset\n",
    "Name = \"British-Coins-Updated-1p-2p/Coins-100-inv_log_freq_2_6_100\"\n",
    "\n",
    "# Frequency Array\n",
    "Frequencies=np.array([119.25])#,178.875,238.5,298.125,357.75,477,596.25,715.5,954,1192.5,1431,1908,2385,2862,3816,4770,5724,7632,9540,12402,16218,20988,26712,34344,43884,57240,73458,95400])\n",
    "Frequencies = Frequencies*6.28 #Convert to rad/s'\n",
    "#Frequencies=np.logspace(2,8,150)\n",
    "#Frequencies=np.logspace(2,6,100)\n",
    "\n",
    "# Classes to Include\n",
    "Classes = [\"British_Coins_Updated_1p_2p\"]\n",
    "#\n",
    "# Global number of results\n",
    "# Coin Problem\n",
    "extend_results = 'global_obj'\n",
    "\n",
    "# Number of secondary results if 'global_obj'\n",
    "\n",
    "#Coin Problem`\n",
    "Num_Results = 100\n",
    "\n",
    "# Classes if 'global_class' or 'classwise'\n",
    "class_split=[]\n",
    "\n",
    "# Name each of the classes this is done in order\n",
    "class_names = []\n",
    "\n",
    "# Number of results if 'global_class'\n",
    "Num_Results_class = []\n",
    "\n",
    "\n",
    "# (int) Number of results per class results per class if 'classwise'\n",
    "class_num_results = []\n",
    "\n",
    "#Labeler False,'Classwise', 'Objectwise'\n",
    "Label_Data = 'Objectwise'\n",
    "Name_Objects = True\n",
    "\n",
    "#(dictionary) name the objects as you wish them to appear in the classificaiton\n",
    "Object_Names_dictionary ={\"Two_Pound\":r\"£2\", \"Ten_Pence\":r\"10p_(new)\", \"Ten_Pence_non_magnetic\":r\"10p_(old)\", \"One_Pound\":r\"£1\", \"Two_Penny\":r\"2p_(new)\",\n",
    "                              \"Two_Penny_non_magnetic\":r\"2p_(old)\", \"Twenty_Pence\":r\"20p\", \"Five_Pence\":r\"5p_(new)\", \"Five_Pence_non_magnetic\":r\"5p_(old)\",\n",
    "                              \"Fifty_Pence\":r\"50p\", \"One_Penny\":r\"1p_(new)\", \"One_Penny_non_magnetic\":r\"1p_(old)\"}\n",
    "\n",
    "\n",
    "Name_Order = [\"One_Penny\",\"One_Penny_non_magnetic\",\"Two_Penny\",\"Two_Penny_non_magnetic\",\"Five_Pence\",\"Five_Pence_non_magnetic\",\n",
    "             \"Ten_Pence\",\"Ten_Pence_non_magnetic\",\"Twenty_Pence\",\"Fifty_Pence\",\"One_Pound\",\"Two_Pound\"]\n",
    "\n",
    "#How to scale\n",
    "Scale_type = 'Global'\n",
    "\n",
    "#Which file (This is not currently used)\n",
    "Scale_File = 'Coin_DataSet.csv'\n",
    "\n",
    "\n",
    "#Alpha scale\n",
    "Alpha_scale = 0.84\n",
    "\n",
    "#Sigma scale\n",
    "Sigma_scale = 12.5\n",
    "\n",
    "# Path Name for where the class data is stored\n",
    "Class_dir = \"Classes_Ben\"\n",
    "\n",
    "# Create a dictionary of these settings\n",
    "Creator_Settings = {\"Name\":Name,\"Frequencies\":Frequencies,\"Classes\":Classes,\"extend_results\":extend_results,\"Num_Results\":Num_Results,#\n",
    "                    \"class_split\":class_split,\"class_names\":class_names,\"Num_Results_class\":Num_Results_class,\"class_num_results\":class_num_results,\\\n",
    "                    \"Label_Data\":Label_Data,\"Name_Objects\":Name_Objects,\"Object_Names_dictionary\":Object_Names_dictionary,\"Name_Order\":Name_Order,\\\n",
    "                    \"Scale_type\":Scale_type,\"Scale_File\":Scale_File,\"Alpha_scale\":Alpha_scale,\"Sigma_scale\":Sigma_scale, \"Class_dir\":Class_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2c8608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder path to the dataset is: British-Coins-Updated-1p-2p/Coins-100-inv_log_freq_2_6_100_Al_0.84_Sig_12.5\n",
      "Class type is a list\n",
      "One_Pound\n",
      "Twenty_Pence\n",
      "Ten_Pence_non_magnetic\n",
      "Fifty_Pence\n",
      "One_Penny\n",
      "Two_Penny_non_magnetic\n",
      "Five_Pence_non_magnetic\n",
      "Ten_Pence\n",
      "One_Penny_non_magnetic\n",
      "Two_Pound\n",
      "Five_Pence\n",
      "Two_Penny\n",
      "(1200, 100) 100\n"
     ]
    }
   ],
   "source": [
    "# Run the Creator script\n",
    "from Creator import *\n",
    "#DataSet_Name=Creator(Name,Frequencies,Classes,extend_results,Num_Results,class_split,class_names,\\\n",
    "#            Num_Results_class,class_num_results,Label_Data, Name_Objects,Object_Names_dictionary,Name_Order,Scale_type,Scale_File,\\\n",
    "#            Alpha_scale,Sigma_scale)\n",
    "DataSet_Name=Creator(Creator_Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96be8df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British-Coins-Updated-1p-2p/Coins-100-inv_log_freq_2_6_100_Al_0.84_Sig_12.5\n"
     ]
    }
   ],
   "source": [
    "# This is the dataset name to be used with the classifier\n",
    "\n",
    "#DataSet_Name=\"British-Coins-Updated-1p-2p/Coins-100_Al_0.84_Sig_12.5\"\n",
    "#DataSet_Name=\"British-Coins-Updated-1p-2p/Coins-100_Al_0.84_Sig_12.5\"\n",
    "#DataSet_Name=\"British-Coins-Updated-1p-2p/Coins-100-inv_Al_0.84_Sig_12.5\"\n",
    "DataSet_Name=\"British-Coins-Updated-1p-2p/Coins-100-inv_log_freq_2_6_100_Al_0.84_Sig_12.5\"\n",
    "print(DataSet_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb993227",
   "metadata": {},
   "source": [
    "**The next part involves running the actual classifier. This is done using Trainer_PDL.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adca0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to load external testing data from disk. Requires that external_file_loader.py be run first.\n",
    "Load_External_Data = False \n",
    "# Option to plot comparison figures between the input array of simulated data and the external test data.\n",
    "# Currently only supported for a single class test set.\n",
    "Plot_Comparison_Figures = True\n",
    "# Option to additionally save to disk: the model for each bootstrap iteration, the normalisation coefficients for each,\n",
    "# bootstrap iteration, and the input array for each model, Used for debugging.\n",
    "Full_Save = False\n",
    "\n",
    "# Option to use SVD to reduce the number of features\n",
    "Reduce_Features = False\n",
    "\n",
    "#Model to be used\n",
    "#Optional models 'LogisticRegression', 'SVM', 'DecisionTree', 'RandomForest', 'GradientBoost', 'MLP','MLP,(n1,n2,...,nn)'\n",
    "Models_to_run = ['LogisticRegression']#,'SVM','GradientBoost']\n",
    "\n",
    "#Features\n",
    "Features = ['Eig1']#['AngleRtildeI']#['Pri1','Pri2','Pri3']#['AngleRtildeI']#\n",
    "# Features = ['Eig1', 'Eig2', 'Eig3']\n",
    "#(list) list of features to be used options:\n",
    "#'Eig1','Eig2','Eig3','Pri1','Pri2','Pri3','Dev2','Dev3','Com','AngleRtildeI'\n",
    "#Eigenvalues, Principal invarients, Deviatoric invarients, Comutator, Angle between Rtilde and I\n",
    "\n",
    "#How many times would you like to train the model\n",
    "Bootstrap_Repetitions = 1\n",
    "#(int) how many times to train the model to obtain an average accuracy\n",
    "\n",
    "# use default levels of SNR\n",
    "SNR_array=[]\n",
    "\n",
    "# Make plot of first two reduced invairants\n",
    "Plot_Principal_Componenets = False\n",
    "\n",
    "Trainer_Settings = {\"DataSet_Name\": DataSet_Name, \"Load_External_Data\":Load_External_Data, \"Plot_Comparison_Figures\":Plot_Comparison_Figures,\\\n",
    "                   \"Full_Save\": Full_Save, \"Reduce_Features\":Reduce_Features, \"Models_to_run\": Models_to_run, \"Features\":Features, #\n",
    "                    \"Bootstrap_Repetitions\": Bootstrap_Repetitions, \"SNR_array\": SNR_array, \"Plot_Principal_Componenets\": Plot_Principal_Componenets }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d510a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level =  5\n",
      "(1200, 1800)\n",
      "(1200, 202) (1200, 200)\n",
      "Data [[8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 3.96141166e-05\n",
      "  5.00810186e-05 6.25333133e-05]\n",
      " [8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 4.41360612e-05\n",
      "  5.54858545e-05 6.88864865e-05]\n",
      " [8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 2.56881742e-05\n",
      "  3.31815180e-05 4.23258681e-05]\n",
      " ...\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.81893873e-04\n",
      "  1.87253084e-04 1.93558228e-04]\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.81763769e-04\n",
      "  1.87085569e-04 1.93373629e-04]\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.82434785e-04\n",
      "  1.87939222e-04 1.94308317e-04]]\n",
      "to here\n",
      "LogisticRegression\n",
      "12 0 200\n",
      "2.5902185587785264e+29\n",
      "Noise level =  10\n",
      "(1200, 1800)\n",
      "(1200, 202) (1200, 200)\n",
      "Data [[8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 3.96141166e-05\n",
      "  5.00810186e-05 6.25333133e-05]\n",
      " [8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 4.41360612e-05\n",
      "  5.54858545e-05 6.88864865e-05]\n",
      " [8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 2.56881742e-05\n",
      "  3.31815180e-05 4.23258681e-05]\n",
      " ...\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.81893873e-04\n",
      "  1.87253084e-04 1.93558228e-04]\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.81763769e-04\n",
      "  1.87085569e-04 1.93373629e-04]\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.82434785e-04\n",
      "  1.87939222e-04 1.94308317e-04]]\n",
      "to here\n",
      "LogisticRegression\n",
      "12 0 200\n",
      "2.3535577960661436e+27\n",
      "Noise level =  15\n",
      "(1200, 1800)\n",
      "(1200, 202) (1200, 200)\n",
      "Data [[8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 3.96141166e-05\n",
      "  5.00810186e-05 6.25333133e-05]\n",
      " [8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 4.41360612e-05\n",
      "  5.54858545e-05 6.88864865e-05]\n",
      " [8.89198896e-08 8.89006449e-08 8.88774636e-08 ... 2.56881742e-05\n",
      "  3.31815180e-05 4.23258681e-05]\n",
      " ...\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.81893873e-04\n",
      "  1.87253084e-04 1.93558228e-04]\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.81763769e-04\n",
      "  1.87085569e-04 1.93373629e-04]\n",
      " [1.30164636e-06 1.30141938e-06 1.30114643e-06 ... 1.82434785e-04\n",
      "  1.87939222e-04 1.94308317e-04]]\n",
      "to here\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTrainer_PDL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#main(DataSet_Name,Load_External_Data,Plot_Comparison_Figures,Full_Save,Reduce_Features,Models_to_run,Features,Bootstrap_Repetitions)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainer_Settings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Coding/MPT-Classifier/MPT_Classifier/Trainer_PDL.py:175\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(Trainer_Settings)\u001b[0m\n\u001b[1;32m    172\u001b[0m         Bootstrap_Repetitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Do not perform bootstrap as we already\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# get output as a probability distribution.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[43mMain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNoise_Levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mDataSet_Name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mLoad_External_Data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mPlot_Comparison_Figures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mFull_Save\u001b[49m\u001b[43m,\u001b[49m\u001b[43mModels_to_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43mBootstrap_Repetitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43mPYCOL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m          \u001b[49m\u001b[43mProbabalistic_Classifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mBayesian_Classifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mScikit_Classifiers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m          \u001b[49m\u001b[43mTenflow_Classifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mProbflow_Classifiers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mReduce_Features\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTrainer_Settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m          \u001b[49m\u001b[43mPlot_Principal_Componenets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Force exit of threads if not properly killed already\u001b[39;00m\n\u001b[1;32m    184\u001b[0m os\u001b[38;5;241m.\u001b[39m_exit(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Coding/MPT-Classifier/MPT_Classifier/Functions/Main_loop.py:225\u001b[0m, in \u001b[0;36mMain_loop\u001b[0;34m(Noise_Levels, DataSet_Name, Load_External_Data, Plot_Comparison_Figures, Full_Save, Models_to_run, Features, Bootstrap_Repetitions, PYCOL, Probabalistic_Classifiers, Bayesian_Classifiers, Scikit_Classifiers, Tenflow_Classifiers, Probflow_Classifiers, Reduce_Features, Trainer_Settings, Plot_Principal_Componenets)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto here\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 225\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mAdd_Noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTraining_noise\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFrequencies\u001b[49m\u001b[43m,\u001b[49m\u001b[43mFeature_Dic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAngles\u001b[49m\u001b[43m,\u001b[49m\u001b[43mAngleFlag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m#Add the noise to the testing data\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Note that this removes the first column of X_test\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(Testing_noise)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Coding/MPT-Classifier/MPT_Classifier/Functions/Noise_adder.py:58\u001b[0m, in \u001b[0;36mAdd_Noise\u001b[0;34m(X_train, Training_noise, Tensors, Features, Frequencies, Feature_Dic, Angles, AngleFlag)\u001b[0m\n\u001b[1;32m     55\u001b[0m NoisyEigenvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;28mlen\u001b[39m(Frequencies),\u001b[38;5;241m3\u001b[39m],dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcomplex\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(Frequencies)):\n\u001b[0;32m---> 58\u001b[0m     Eigenvalues[j,:] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(\u001b[43mLA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     59\u001b[0m     Eigenvalues[j,:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;241m1\u001b[39mj\u001b[38;5;241m*\u001b[39mLA\u001b[38;5;241m.\u001b[39meig(Tensor[j,:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mimag)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     60\u001b[0m NoisyEigenvalues[:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m AGWNr(np\u001b[38;5;241m.\u001b[39mreal(Eigenvalues[:,\u001b[38;5;241m0\u001b[39m]),Training_noise)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39mj\u001b[38;5;241m*\u001b[39mAGWNr(np\u001b[38;5;241m.\u001b[39mimag(Eigenvalues[:,\u001b[38;5;241m0\u001b[39m]),Training_noise)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36meig\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/mpt_classifier/lib/python3.8/site-packages/numpy/linalg/linalg.py:1297\u001b[0m, in \u001b[0;36meig\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1295\u001b[0m a, wrap \u001b[38;5;241m=\u001b[39m _makearray(a)\n\u001b[1;32m   1296\u001b[0m _assert_stacked_2d(a)\n\u001b[0;32m-> 1297\u001b[0m \u001b[43m_assert_stacked_square\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m _assert_finite(a)\n\u001b[1;32m   1299\u001b[0m t, result_t \u001b[38;5;241m=\u001b[39m _commonType(a)\n",
      "File \u001b[0;32m~/anaconda3/envs/mpt_classifier/lib/python3.8/site-packages/numpy/linalg/linalg.py:187\u001b[0m, in \u001b[0;36m_assert_stacked_square\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_stacked_square\u001b[39m(\u001b[38;5;241m*\u001b[39marrays):\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays:\n\u001b[1;32m    188\u001b[0m         m, n \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;241m!=\u001b[39m n:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Trainer_PDL import *\n",
    "\n",
    "#main(DataSet_Name,Load_External_Data,Plot_Comparison_Figures,Full_Save,Reduce_Features,Models_to_run,Features,Bootstrap_Repetitions)\n",
    "main(Trainer_Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b5a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f4d7e-007a-470d-a072-3910c7c28cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
