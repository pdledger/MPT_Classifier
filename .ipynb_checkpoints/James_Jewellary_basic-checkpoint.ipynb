{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067fe755",
   "metadata": {},
   "source": [
    "**The first thing to do is to run Creator.py to setup the dataset. First specify the options**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32ddb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# The Name of the Dataset\n",
    "Name = 'Class_5/Class_5_400'\n",
    "\n",
    "# Frequency Array\n",
    "Frequencies=np.array([119.25,178.875,238.5,298.125,357.75,477,596.25,715.5,954,1192.5,1431,1908,2385,2862,3816,4770,5724,7632,9540,12402,16218,20988,26712,34344,43884,57240,73458,95400])\n",
    "Frequencies = Frequencies*6.28 #Convert to rad/s'\n",
    "\n",
    "# Classes to Include\n",
    "# In this example we use all types of Jewelery, but no other classes and so the class to include si just this one.\n",
    "#Classes = [\"Jewlery\"]\n",
    "Classes = [\"Jewellery\"]\n",
    "#(list) list of strings for the classes to be included in the training set\n",
    "#this is hierarchic so includes all subclasses of a given class. Can also\n",
    "#be 'all' to include all classes.\n",
    "\n",
    "# Global number of results\n",
    "#8 and 15 class problem\n",
    "extend_results = 'global_class'\n",
    "#(string) 'global_obj','global_class' or 'classwise' if 'objectwise' required use scaling file\n",
    "\n",
    "\n",
    "# Number of secondary results if 'global_obj'\n",
    "Num_Results = 400\n",
    "\n",
    "# Classes if 'global_class' or 'classwise'\n",
    "#5 class\n",
    "# Ben's classes\n",
    "#class_split = ['Bracelets','Watches']#,'Earings','Pendants','Rings']\n",
    "#James classes (with corrected spellings)\n",
    "class_split = ['Bracelets','Watches','Earrings','Pendants','Rings']\n",
    "\n",
    "\n",
    "# Name each of the classes this is done in order\n",
    "# 5 class\n",
    "class_names = ['Bracelets','Watches','Earings','Pendants','Rings']\n",
    "\n",
    "# Number of results if 'global_class'\n",
    "# 5 class problem\n",
    "Num_Results_class = 400\n",
    "\n",
    "# (int) Number of results per class results per class if 'classwise'\n",
    "class_num_results = [400,400,400,400,400]#[150,100,100,100,200,100,100,300]\n",
    "\n",
    "#Labeler False,'Classwise', 'Objectwise'\n",
    "#8 and 15 class\n",
    "Label_Data = 'Classwise'\n",
    "#(boolean or string) create training labels for the dataset\n",
    "\n",
    "#If Label_Data = 'Objectwise' you can create a dictionary of names for each object\n",
    "Name_Objects = []\n",
    "\n",
    "#(dictionary) name the objects as you wish them to appear in the classificaiton\n",
    "Object_Names_dictionary =[]\n",
    "\n",
    "Name_Order = ['Bracelets','Watches','Earings','Pendants','Rings']\n",
    "\n",
    "\n",
    "#How to scale\n",
    "Scale_type = 'Global'\n",
    "\n",
    "#Which file (This is not currently used)\n",
    "Scale_File = 'Coin_DataSet.csv'\n",
    "\n",
    "\n",
    "#Alpha scale\n",
    "Alpha_scale = 0.84 # original scaling\n",
    "\n",
    "#Sigma scale\n",
    "Sigma_scale = 2.4 # original scaling\n",
    "\n",
    "#From paper s_alpha = 8.4e-6 to obtain: Alpha_scale = (s_alpha/alpha)*100\n",
    "#From paper s_sigma = 9.52e5 to obtain: Sigma_scale = (s_sigma/sigma)*100\n",
    "\n",
    "# Create a dictionary of these settings\n",
    "Creator_Settings = {\"Name\":Name,\"Frequencies\":Frequencies,\"Classes\":Classes,\"extend_results\":extend_results,\"Num_Results\":Num_Results,#\n",
    "                    \"class_split\":class_split,\"class_names\":class_names,\"Num_Results_class\":Num_Results_class,\"class_num_results\":class_num_results,\\\n",
    "                    \"Label_Data\":Label_Data,\"Name_Objects\":Name_Objects,\"Object_Names_dictionary\":Object_Names_dictionary,\"Name_Order\":Name_Order,\\\n",
    "                    \"Scale_type\":Scale_type,\"Scale_File\":Scale_File,\"Alpha_scale\":Alpha_scale,\"Sigma_scale\":Sigma_scale}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "096a9b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder path to the dataset is: Class_5/Class_5_400_Al_0.84_Sig_2.4\n"
     ]
    }
   ],
   "source": [
    "# Run the Creator script\n",
    "from Creator import *\n",
    "DataSet_Name=Creator(Creator_Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a199d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_5/Class_5_400_Al_0.84_Sig_2.4\n"
     ]
    }
   ],
   "source": [
    "# This is the dataset name to be used with the classifier\n",
    "print(DataSet_Name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7abf85",
   "metadata": {},
   "source": [
    "**The next part involves running the actual classifier. This is done using Trainer_PDL.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2695d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to load external testing data from disk. Requires that external_file_loader.py be run first.\n",
    "Load_External_Data = False \n",
    "# Option to plot comparison figures between the input array of simulated data and the external test data.\n",
    "# Currently only supported for a single class test set.\n",
    "Plot_Comparison_Figures = False\n",
    "# Option to additionally save to disk: the model for each bootstrap iteration, the normalisation coefficients for each,\n",
    "# bootstrap iteration, and the input array for each model, Used for debugging.\n",
    "Full_Save = False\n",
    "\n",
    "# Option to use SVD to reduce the number of features\n",
    "Reduce_Features = True\n",
    "\n",
    "#Model to be used\n",
    "#Optional models 'LogisticRegression', 'SVM', 'DecisionTree', 'RandomForest', 'GradientBoost', 'MLP','MLP,(n1,n2,...,nn)'\n",
    "Models_to_run = ['GradientBoost']\n",
    "\n",
    "#Features\n",
    "Features = ['Pri1', 'Pri2', 'Pri3']\n",
    "# Features = ['Eig1', 'Eig2', 'Eig3']\n",
    "#(list) list of features to be used options:\n",
    "#'Eig1','Eig2','Eig3','Pri1','Pri2','Pri3','Dev2','Dev3','Com'\n",
    "#Eigenvalues, Principal invarients, Deviatoric invarients, Comutator\n",
    "\n",
    "#How many times would you like to train the model\n",
    "Bootstrap_Repetitions = 1\n",
    "#(int) how many times to train the model to obtain an average accuracy\n",
    "\n",
    "# use default levels of SNR\n",
    "SNR_array=[]\n",
    "\n",
    "Trainer_Settings = {\"DataSet_Name\": DataSet_Name, \"Load_External_Data\":Load_External_Data, \"Plot_Comparison_Figures\":Plot_Comparison_Figures,\\\n",
    "                   \"Full_Save\": Full_Save, \"Reduce_Features\":Reduce_Features, \"Models_to_run\": Models_to_run, \"Features\":Features, \"Bootstrap_Repetitions\": Bootstrap_Repetitions, \"SNR_array\": SNR_array }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cf0c8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise level =  5\n",
      "[3.18185512e-07 3.31219529e-07 2.93252364e-07 ... 4.67350042e-06\n",
      " 4.81956503e-06 5.02435527e-06]\n",
      "(2061, 170) (2061, 168)\n",
      "to here\n",
      "GradientBoost\n",
      "1545 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  111\n",
      "(1545, 111)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 516  Results\n",
      "(516, 5) (516, 5) (516, 5)\n",
      "Completed snapshot classification\n",
      "Noise level =  10\n",
      "[3.18185512e-07 3.31219529e-07 2.93252364e-07 ... 4.67350042e-06\n",
      " 4.81956503e-06 5.02435527e-06]\n",
      "(2061, 170) (2061, 168)\n",
      "to here\n",
      "GradientBoost\n",
      "1545 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  109\n",
      "(1545, 109)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 516  Results\n",
      "(516, 5) (516, 5) (516, 5)\n",
      "Completed snapshot classification\n",
      "Noise level =  15\n",
      "[3.18185512e-07 3.31219529e-07 2.93252364e-07 ... 4.67350042e-06\n",
      " 4.81956503e-06 5.02435527e-06]\n",
      "(2061, 170) (2061, 168)\n",
      "to here\n",
      "GradientBoost\n",
      "1545 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  107\n",
      "(1545, 107)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 516  Results\n",
      "(516, 5) (516, 5) (516, 5)\n",
      "Completed snapshot classification\n",
      "Noise level =  20\n",
      "[3.18185512e-07 3.31219529e-07 2.93252364e-07 ... 4.67350042e-06\n",
      " 4.81956503e-06 5.02435527e-06]\n",
      "(2061, 170) (2061, 168)\n",
      "to here\n",
      "GradientBoost\n",
      "1545 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  106\n",
      "(1545, 106)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 516  Results\n",
      "(516, 5) (516, 5) (516, 5)\n",
      "Completed snapshot classification\n",
      "Noise level =  40\n",
      "[3.18185512e-07 3.31219529e-07 2.93252364e-07 ... 4.67350042e-06\n",
      " 4.81956503e-06 5.02435527e-06]\n",
      "(2061, 170) (2061, 168)\n",
      "to here\n",
      "GradientBoost\n",
      "1545 168\n",
      " SVD complete      \n",
      "Reduced the number of features from 168  to  88\n",
      "(1545, 88)\n",
      "Completed predictions\n",
      "Completed plotting\n",
      "Found 516  Results\n",
      "(516, 5) (516, 5) (516, 5)\n",
      "Completed snapshot classification\n"
     ]
    }
   ],
   "source": [
    "from Trainer_PDL import *\n",
    "\n",
    "main(Trainer_Settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e38698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e612800-6dc2-45de-ad19-af37ed268c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fbca18-ff3b-49a4-9665-398e50cdb6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b26a23-b998-4e4f-8b4c-b83fbfc1d395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
